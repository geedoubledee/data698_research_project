---
title: "DATA698: Research Project: Data Analysis"
author: "Glen Dale Davis"
date: "2024-03-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages

```{r packages, warning = FALSE, message = FALSE}
library(caret)
library(elasticnet)
library(e1071)
library(factoextra)
library(glmnet)
library(jsonlite)
library(knitr)
library(Matrix)
library(naivebayes)
library(packrat)
library(RColorBrewer)
library(snakecase)
library(sparcl)
library(stats)
library(stopwords)
library(tidytext)
library(tidyverse)
library(topicmodels)
library(UpSetR)
library(xgboost)

```

## Introduction

We load recipe review data that we gathered from AllRecipes.com in early 2024. The reviews we gathered included each recipe's "Most Helpful Positive Review," which is the first review that Allrecipes.com shows users and is determined by user input, as well as each recipe's nine most recent reviews. Some recipes didn't have any positive reviews marked helpful by other users, so their "Most Helpful Positive Review" field was blank, and some recipes had been reviewed less than nine times. So there is more review data available for some recipes than others.

```{r data}
base <- "https://raw.githubusercontent.com/geedoubledee/data698_research_project/main/data/allrecipes_df_pt_"
exts <- c("1.csv", "2.csv", "3.csv", "4.csv", "5.csv")
for (i in 1:length(exts)){
    url <- paste0(base, exts[i])
    df <- read.csv(url)
    if (i == 1){
        ar_df <- df
    }else{
        ar_df <- ar_df |>
            bind_rows(df)
    }
}
url <- "https://raw.githubusercontent.com/geedoubledee/data698_research_project/main/data/links_df.csv"
links_df <- read.csv(url)

```

Home cooks of all skill levels frequent AllRecipes.com for meal inspiration, but the quality of user-submitted recipe content on the site varies widely. We have identified nine kinds of errors (related to missing or misleading steps, incorrect measurements, incorrect cooking or preparation time, incorrect temperature, a key ingredient being missing, a flavor ingredient being missing, an incorrect ingredient being included, and user error) that recipes might suffer from. These errors are generally evident in the reviews related to a recipe, although different users frequently highlight different errors or propose alternative solutions to the same error.

Our goal is to classify reviews according to the kinds of errors they suggest the recipe suffers from.

## Exploratory Data Analysis

***TK***

## Initial Data Preparation for Unsupervised Learning

During the data collection process, bad URLs were given a `Visited` value of -1 to separate them from good links we had already visited (value: 1) and links we had not yet visited (value: 0). Some of these bad links were added to the records erroneously, so we identify and remove them. 

```{r bad_links}
ar_df <- ar_df |>
    left_join(links_df, by = join_by(URL)) |>
    filter(Visited != -1) |>
    select(-Visited)
rm(links_df)

```

We also intended to exclude any recipes that had never been reviewed from the data collection process, but a small number of them did make their way into the records, so we remove those as well. 

```{r no_reviews}
ar_df <- ar_df |>
    filter(!is.na(Reviews_Total))

```

While some positive reviews may identify problems with a recipe, it is reasonable to assume that negative reviews are more likely to do so. So we first create a corpus of potentially negative review text. This will make manually labeling reviews by what problem they are identifying faster later by hopefully limiting the number of reviews we will have to look at that don't identify any problems at all. We define a potentially negative review as any text review combined with a star rating of 3 or less.

We pivot our wide review data into a longer format, dropping some columns we don't need for this stage of the analysis. 

```{r pivot_long}
keep <- c("URL", "Recipe_Name", "Ratings_Total", "Ratings_Avg", "Reviews_Total")
user_sub <- ar_df |>
    select(c(all_of(keep), starts_with("User_"))) |>
    pivot_longer(cols = starts_with("User_"), names_to = "Review_Num",
                 values_to = "User", names_prefix = "User_")
rating_sub <- ar_df |>
    select(c("URL", starts_with("Rating_"))) |>
    pivot_longer(cols = starts_with("Rating_"), names_to = "Review_Num",
                 values_to = "Rating", names_prefix = "Rating_")
date_sub <- ar_df |>
    select(c("URL", starts_with("Date_"))) |>
    pivot_longer(cols = starts_with("Date_"), names_to = "Review_Num",
                 values_to = "Date", names_prefix = "Date_")
txt_sub <- ar_df |>
    select(-starts_with("Review_Len")) |>
    select(c("URL", starts_with("Review_"))) |>
    pivot_longer(cols = starts_with("Review_"), names_to = "Review_Num",
                 values_to = "Review_Txt", names_prefix = "Review_")
txt_len_sub <- ar_df |>
    select(c("URL", starts_with("Review_Len"))) |>
    pivot_longer(cols = starts_with("Review_Len"), names_to = "Review_Num",
                 values_to = "Review_Len", names_prefix = "Review_Len")
helpful_sub <- ar_df |>
    select(c("URL", starts_with("Helpful_"))) |>
    pivot_longer(cols = starts_with("Helpful_"), names_to = "Review_Num",
                 values_to = "Helpful", names_prefix = "Helpful_")
ar_df_long <- user_sub |>
    left_join(rating_sub, by = join_by(URL, Review_Num)) |>
    left_join(date_sub, by = join_by(URL, Review_Num)) |>
    left_join(txt_sub, by = join_by(URL, Review_Num)) |>
    left_join(txt_len_sub, by = join_by(URL, Review_Num)) |>
    left_join(helpful_sub, by = join_by(URL, Review_Num)) |>
    filter(!is.na(Rating))
print(nrow(ar_df_long))

```

This leaves us with 134,800 text reviews. We remove any reviews with star ratings of 4 or higher.

```{r }
ar_df_long_neg <- ar_df_long |>
    filter(Rating < 4)
nrows <- nrow(ar_df_long_neg)
print(nrows)

```

That leaves us with 14,954 potentially negative reviews.

## Unsupervised Learning: Topic Modeling

We start by fuzzy clustering the reviews into topics using bigrams. We chose bigrams because they include more context than unigrams, and some words that are commonly considered stopwords for text data might actually be useful for our analysis. So we do not exclude bigrams that contain words like "too" and "not" from consideration. Many reviews include text like "too much" or "not enough," both of which could indicate measurement errors, and we hope fuzzy clustering will group reviews together based on similar informative language like this. We do eliminate bigrams containing non-useful stopwords (e.g. pronouns), as well as bigrams containing numbers. (We noticed later that our method unintentionally left bigrams containing fractions in.)

```{r warning = FALSE, message = FALSE}
stop_w <- as.data.frame(stopwords(language = "en", source = "nltk"))
colnames(stop_w) <- c("WORD")
excl <- c("do", "does", "did", "doing", "during", "before", "after", "up",
          "down", "over", "under", "when", "why", "how", "all", "any", "both",
          "each", "few", "more", "most", "no", "nor", "not", "only", "than",
          "too", "very", "can", "will", "don't", "should", "should've",
          "aren't", "couldn't", "didn't", "doesn't", "hadn't", "hasn't",
          "haven't", "isn't", "mightn't", "mustn't", "needn't", "shan't",
          "shouldn't", "wasn't", "weren't", "won't", "wouldn't")
stop_w <- stop_w |>
    mutate(SRC = "NLTK") |>
    filter(!WORD %in% excl)
keep <- c("URL", "Review_Num", "Review_Txt")
names <- c("w1", "w2")
ar_df_long_neg_bigrams <- ar_df_long_neg |>
    select(all_of(keep)) |>
    unnest_tokens(output = BIGRAM, input = Review_Txt,
                  token = "ngrams", n = 2) |>
    count(URL, Review_Num, BIGRAM, name = "COUNT") |>
    ungroup() |>
    separate_wider_delim(BIGRAM, delim = " ", names = names,
                         cols_remove = FALSE) |>
    mutate(w1 = ifelse(w1 %in% stop_w$WORD, NA, w1),
           w1 = ifelse(!is.na(as.numeric(w1)), NA, w1),
           w2 = ifelse(w2 %in% stop_w$WORD, NA, w2),
           w2 = ifelse(!is.na(as.numeric(w2)), NA, w2)) |>
    filter(!is.na(w1) & !is.na(w2)) |>
    select(-all_of(names)) |>
    mutate(BIGRAM = to_snake_case(BIGRAM))
ar_df_long_neg_bigrams_dtm <- ar_df_long_neg_bigrams |>
    unite("ID", URL, Review_Num, sep = "_Review_Num_") |>
    cast_dtm(ID, BIGRAM, COUNT)

```

Using values of k from two to 12, we measure the perplexity of several different Latent Dirichlet Allocation (LDA) models that would automatically divide the data into k topics. We plot the perplexity measures against the number of topics.

```{r }
get_perplexity <- function(dtm, k_max){
    #function adapted from https://bit.ly/4cLUHO8
    m <- matrix(,1,1)
    for (i in 2:k_max){
        lda <- LDA(dtm, i)
        per <- perplexity(lda)
        m <- plyr::rbind.fill.matrix(m, per)
    }
    m <- as.data.frame(cbind(matrix(seq(2, k_max), ncol = 1), m[-1,]))
    colnames(m) <- c("topics", "perplexity")
    return(m)
}
fn <- "data/perplexity_values.csv"
if (!file.exists(fn)){
    per_vals <- get_perplexity(ar_df_long_neg_bigrams_dtm, k_max = 12)
    write.csv(per_vals, fn, row.names = FALSE, fileEncoding = "UTF-8")
}else{
    per_vals <- read.csv(fn)
}
cur_theme <- theme_set(theme_classic())
pal <- brewer.pal(12, "Paired")
p0 <- per_vals |>
    ggplot(aes(x = topics, y = perplexity)) +
    geom_line(color = pal[9]) +
    geom_point(color = pal[10]) +
    scale_x_continuous(breaks = seq(2, 12, 1), limits = c(2, 12)) +
    scale_y_continuous(breaks = seq(0, 25000, 5000), limits = c(0, 25000))
p0

```

We see that the reduction in perplexity begins to drop off after about six topics, so we set k to six in the final LDA model. We plot the bigrams that are most frequent in the six identified topics. 

```{r }
fn <- "ar_lda.rds"
if (!file.exists(fn)){
    ar_lda <- LDA(ar_df_long_neg_bigrams_dtm, k = 6, control = list(seed = 208))
    saveRDS(ar_lda, "ar_lda.rds")
}else{
    ar_lda <- readRDS("ar_lda.rds")
}
ar_topics <- tidy(ar_lda, matrix = "beta")
top_terms <- ar_topics |>
    group_by(topic) |>
    slice_max(beta, n = 15) |>
    ungroup() |>
    arrange(topic, -beta) |>
    mutate(term = reorder_within(term, beta, topic))
p1 <- top_terms |>
    ggplot(aes(beta, term)) +
    geom_col(color = pal[10], fill = pal[9]) +
    facet_wrap(~ topic, scales = "free_y", nrow = 2) +
    scale_y_reordered()
p1


```

We see a lot of commonality in the bigrams that are frequent within these six topics. For instance, "next time" is the first or second most identifying bigram in four out of six topics, indicating the reviews in each of these topics are likely to exhibit some disappointment in the results and some suggestions for improving those results in the future. Despite all the overlap, there are also bigrams that are unique to the top identifiers for each topic though.

It's notable that some of these unique bigrams are related to specific ingredients, i.e. brown sugar being an identifier of topic one, cream cheese being an identifier of topics two and four, and soy sauce being an identifier of topic six. Ingredients have therefore resulted in some unintended grouping. We check for an additional instance of unintended grouping since the reviews are of various lengths, and it is reasonable to suspect review length might have incidentally factored into the topic decision boundaries as well. 

```{r }
ar_gamma <- tidy(ar_lda, matrix = "gamma")
names <- c("URL", "Review_Num")
ar_classifications <- ar_gamma |>
    separate_wider_delim(document, delim = "_Review_Num_", names = names) |>
    group_by(URL, Review_Num) |>
    slice_max(gamma) |>
    ungroup() |>
    mutate(topic = as.factor(topic))
incl <- c("URL", "Review_Num", "Review_Len")
rev_len_by_top <- ar_classifications |>
    left_join(ar_df_long_neg |> select(all_of(incl)),
              by = join_by(URL, Review_Num))
annotations <- rev_len_by_top |>
    group_by(topic) |>
    summarize(`1st Qu.:` = round(quantile(Review_Len, probs = 0.25), 0),
              `Med.:` = median(Review_Len),
              `3rd Qu.:` = round(quantile(Review_Len, probs = 0.75), 0)) |>
    pivot_longer(cols = !topic, names_to = "Variable",
                 values_to = "Value")
p2 <- rev_len_by_top |>
    ggplot(aes(x = Review_Len)) + 
    geom_histogram(binwidth = 100, color = pal[10], fill = pal[9]) +
    geom_segment(aes(x = Value, xend = Value, y = 0, yend = 1000),
               data = annotations |> filter(Variable == "Med.:"),
               linetype="dashed", color = "black", linewidth = 0.5) +
    geom_text(aes(x = Value, y = 900, label = paste(Variable, Value)),
              data = annotations |> filter(Variable == "Med.:"),
              nudge_x = 450, color = "black") + 
    scale_y_continuous(breaks = seq(0, 1000, 250), limits = c(0, 1000)) +
    facet_wrap(~ topic, nrow = 3) +
    labs(x = "Review Length (Characters)", y = "Frequency",
         title = "Distribution of Review Length by Topic")
p2

```

The topics actually have pretty similar review length distributions. Topic 4 has the highest median review length at 201 characters, but that is only 22 more characters than the lowest median review length (179 characters, shared by topics three and six).

## Data Labeling

We export the potentially negative reviews for manual labeling, then load said labeled data. 

```{r }
fn <- "data/allrecipes_df_neg_labs.csv"
if (!file.exists(fn)){
    ar_df_long_neg_labs <- ar_df_long_neg |>
        left_join(ar_classifications, by = join_by(URL, Review_Num)) |>
        arrange(topic, desc(gamma))
    write.csv(ar_df_long_neg_labs, fn, row.names = FALSE, fileEncoding = "UTF-8")
}else{
    ar_df_long_neg_labs <- read.csv(fn)
}
ar_df_long_neg_labs <- ar_df_long_neg_labs |>
    mutate(topic = ifelse(is.na(topic), 0, topic),
           gamma = ifelse(is.na(gamma), 0, gamma)) |>
    na.omit()
print(nrow(ar_df_long_neg_labs))

```

There are now 3,398 labeled observations in total: 515 observations from each of the six topics identified during LDA, plus 308 observations that were not composed of any significant bigrams and were thus assigned a topic of `NA`. (We have recoded `NA` topic values as 0.) Reasons why a review might not be composed of any significant bigrams include:

* the review was devoid of any words (e.g. "!!!" )

* the review contained only stopwords (e.g. "so so")

* all possible bigrams in the review contained stopwords (e.g. "I hated it")

The observations have been labeled in two ways:

1) as either "Neutral" or "Negative" in `SENTIMENT`

2) by which of nine specific problems they identify (if any): `DIRECTIONS`, `MEASUREMENTS`, `TIMING`, `TEMPERATURE`, `ING_KEY_MISSING`, `ING_FLAVOR_MISSING`, `ING_WRONG_INCL`, `INAUTHENTIC`, and `USER_ERROR`

These nine specific problems and descriptions of the criteria used to define whether a review identified them are outlined below:

```{r }
prob <- c("DIRECTIONS", "MEASUREMENTS", "TIMING", "TEMPERATURE", "ING_KEY_MISSING", "ING_FLAVOR_MISSING", "ING_WRONG_INCL", "INAUTHENTIC", "USER_ERROR")
desc <- c("One or more steps was unclear, incorrect, or missing",
          "Included too much or too little of one or more ingredients",
          "Needed more/less time to prep or to finish cooking",
          "Needed higher/lower temperature to cook or assemble correctly",
          "Missing a key ingredient",
          "Missing a flavor ingredient",
          "Included an incorrect ingredient",
          "Lacked a resemblance to the dish's traditional preparation/flavor",
          "Reviewer suspected/admitted they made a mistake")
tbl <- cbind(prob, desc)
colnames(tbl) <- c("PROBLEM", "DESCRIPTION")
kable(tbl, format = "simple")

```

Because we only looked at reviews with star ratings of 3 or less, we originally felt it should not be possible for these reviews to receive a "Positive" label for `SENTIMENT`. However, we did encounter review text along the way that was clearly neither "Negative" nor "Neutral," and it's possible the reviewer intended to leave a higher star rating along with their seemingly "Positive" words. For the sake of consistency, we continued to categorize these "Positive" reviews as "Neutral" during labeling so that we could redefine our `SENTIMENT` variable later and hopefully eliminate any confusion. To that end, we now replace the `SENTIMENT` variable with the `NEGATIVE` variable, a binary predictor in which a value of 1 indicates the review text expressed a "Negative" sentiment, and a value of 0 simply indicates it did not. 

```{r }
ar_df_long_neg_labs <- ar_df_long_neg_labs |>
    rename(NEGATIVE = SENTIMENT) |>
    mutate(NEGATIVE = ifelse(NEGATIVE == "Negative", 1, 0))

```

We visualize the frequency of "Negative" reviews by star rating.

```{r }
sel <- c("Negative", "Other")
sent_star_summ_df <- ar_df_long_neg_labs |>
    filter(Rating > 0) |>
    group_by(Rating) |>
    summarize(Total = n(),
              Negative = sum(NEGATIVE)) |>
    ungroup() |>
    mutate(Rating = paste0(Rating, "★"),
           Other = Total - Negative) |>
    pivot_longer(cols = all_of(sel),
                 names_to = "Category", values_to = "Value") |>
    mutate(Perc = paste0(round(Value / Total * 100, 1), "%"))
greys <- brewer.pal(9, "Greys")
col <- c(pal[8], greys[6])
fil <- c(pal[7], greys[4])
title_str <- "Frequency of Reviews Expressing Negative Sentiment by Star Rating"
p3 <- sent_star_summ_df |>
    ggplot(aes(x = Category, y = Value,
               group = Category, color = Category, fill = Category)) +
    geom_col() +
    geom_text(aes(label = Perc), vjust = -0.75,
              size = 4, fontface = "bold") +
    scale_y_continuous(limits = c(0, 2000), breaks = seq(0, 2000, 250)) +
    scale_color_manual(values = col) +
    scale_fill_manual(values = fil) +
    facet_grid(~ Rating, scales = "free_x", space = "free_x", switch = "y") +
    labs(x = "Sentiment", y = "Frequency",
         title = title_str) +
    theme(legend.position = "none", panel.spacing = unit(0, units = "cm"),
          strip.placement = "inside",
          strip.background = element_blank(),
          strip.text = element_text(size = 12, face = "bold"),
          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
          plot.title.position = "plot")
p3

```

In line with our expectations, most of the 1- and 2-star reviews we labeled expressed "Negative" sentiment, whereas most of the 3-star reviews we expressed did not. We also encountered more than twice as many 3-star reviews as we did 1- or 2-star reviews during labeling, but 1- and 2-star reviews are similar enough categories that we can justifiably aggregate them. When we consider them in aggregate, the number of 1- and 2-star reviews we labeled is similar to the number of 3-star reviews we labeled.

We now visualize the frequency and some of the overlap of the nine specific problems we identified. 

```{r }
p4 <- ar_df_long_neg_labs |>
    upset(sets = prob, group.by = "sets", cutoff = 4,
          matrix.color = pal[10], main.bar.color = pal[10],
          sets.bar.color = pal[10], text.scale = 1, show.numbers = "no")
p4

```

Reviews that identify `MEASUREMENTS` errors are most frequent, whereas reviews that identify `TEMPERATURE` errors are least frequent. When reviews mention two problems, `MEASUREMENTS` is frequently one of them. It is also worth noting that some of the problems frequently occur together, as well as with authenticity problems. Lastly, `TEMPERATURE` errors occur frequently with `TIMING` errors, which is sensible because timing issues can often be solved with temperature changes, and vice versa.

We visualize the distribution of problems by topic.

```{r }
sel <- c("topic", prob)
prob_by_top <- ar_df_long_neg_labs |>
    select(all_of(sel)) |>
    group_by(topic) |>
    summarize(across(all_of(prob), ~ sum(.x))) |>
    pivot_longer(cols = !topic, names_to = "Problem", values_to = "Count")
p5 <- prob_by_top |>
    filter(topic > 0) |>
    ggplot(aes(x = reorder(Problem, desc(Count)), y = Count)) + 
    geom_bar(stat = "identity", color = pal[10], fill = pal[9]) +
    labs(x = "Problem", y = "Frequency",
         title = "Distribution of Problems by Topic") +
    coord_flip() +
    facet_wrap(~ topic, nrow = 3)
p5

```

While there are some visible differences in the exact frequencies of problems by topic, it is clear that LDA did not segment the topics by the problems, or there would be topics where particular problems are relatively absent and others are relatively dominant. This is not terribly surprising since the problems are non-exclusive, and we only did a moderate amount of data preparation prior to trying to identify the topics before we did manual labeling. 

Since we anticipated unsupervised learning would be insufficient for our problem classification needs, and we primarily used it in hopes of speeding the manual labeling process along, we now proceed with preparing the labeled data further for supervised learning.

## Secondary Data Preparation for Supervised Learning

First, we collapse the three ingredient-related problems into a single problem. While the specificity with which we originally labeled the data regarding ingredient issues might be helpful for other analyses, ours will benefit from being slightly less complex. 

```{r }
ing <- c("ING_KEY_MISSING", "ING_FLAVOR_MISSING", "ING_WRONG_INCL")
ar_df_long_neg_labs <- ar_df_long_neg_labs |>
    mutate(INGREDIENTS = rowSums(across(all_of(ing))),
           INGREDIENTS = ifelse(INGREDIENTS > 0, 1, 0)) |>
    select(-all_of(ing)) |>
    relocate(INGREDIENTS, .before = INAUTHENTIC)

```

The `INGREDIENTS` response variable now takes the place of the `ING_KEY_MISSING`, `ING_FLAVOR_MISSING`, and `ING_WRONG_INCL` response variables. A value of 1 for the new variable indicates there was at least one ingredient issue, whereas a value of 0 indicates there were no ingredient issues. We reprint the problems/criteria table to reflect this change.

```{r }
prob <- c("DIRECTIONS", "MEASUREMENTS", "TIMING", "TEMPERATURE",
          "INGREDIENTS", "INAUTHENTIC", "USER_ERROR")
desc <- c("One or more steps was unclear, incorrect, or missing",
          "Included too much or too little of one or more ingredients",
          "Needed more/less time to prep or to finish cooking",
          "Needed higher/lower temperature to cook or assemble correctly",
          "Missing a key/flavor ingredient or included an incorrect ingredient",
          "Lacked a resemblance to the dish's traditional preparation/flavor",
          "Reviewer suspected/admitted they made a mistake")
tbl <- cbind(prob, desc)
colnames(tbl) <- c("PROBLEM", "DESCRIPTION")
kable(tbl, format = "simple")

```

Next, we refine the text pre-processing and tokenization that was done prior to LDA topic modeling. 

First, we replace any ingredients a review mentions with a single token: "__ING__." We will be better able to group similar language together this way because we want to know how frequently a review discusses ingredients and the contexts in which ingredients are being discussed, but it doesn't matter which ingredient a reviewer is saying there is "too much" of for our purposes. The ingredients list we are using to perform this reduction in complexity is neither perfect nor exhaustive, but it covers enough common ingredients to be useful.

We also replace any numbers (including integers, fractions, mixed fractions, and decimals) with a single token: "__NUM__," we remove any punctuation marks except for apostrophes (after replacing right single quotation marks used as such), and we convert everything to lowercase. 

```{r }
my_url1 <- "https://raw.githubusercontent.com/geedoubledee/data698_research_project/main/data/recipe_ing_train.json"
ings <- read_json(my_url1)
copy <- ings
for (i in 1:length(copy)){
    l <- copy[[i]]
    ings[[i]] <- l$ingredients
}
my_url2 <- "https://raw.githubusercontent.com/geedoubledee/data698_research_project/main/data/recipe_ing_test.json"
ings2 <- read_json(my_url2)
copy <- ings2
for (i in 1:length(copy)){
    l <- copy[[i]]
    ings2[[i]] <- l$ingredients
}
ings <- as.list(unique(sort(c(unlist(ings), unlist(ings2)), decreasing = TRUE)))
rm(ings2, copy)
ings <- gsub("[^[:alpha:] ]", "", ings)
ings <- lapply(ings, str_squish)
ings <- lapply(ings, tolower)
ings <- lapply(ings, function(x) paste0(" ", x, " "))
ing_patt_string <- paste(unlist(ings), collapse = "|")
int_patt <- c("\\s\\d+\\s(?!\\d)")
mixed_frac_patt <- c("\\s\\d+\\s\\d+slash\\d+\\s")
frac_patt <- c("\\s\\d+slash\\d+\\s")
dec_patt <- c("\\s\\d+point\\d+\\s")
num_patt_string <- paste(int_patt, mixed_frac_patt, frac_patt, dec_patt,
                         sep = "|")
#adapted from https://en.wikipedia.org/wiki/Cooking_weights_and_measures
meas <- list("drops", "drop", "drs", "dr", "smidgens", "smidgen", "smdgs", "smdg", "smis", "smi", "pinches", "pinch", "pns", "pn", "dashes", "dash", "ds", "teaspoons", "teaspoon", "tsps", "tsp", "t", "tablespoons", "tablespoon", "tbsps", "tbsp", "ounces", "ounce", "oz", "cups", "cup", "c", "pints", "pint", "pts", "pt", "quarts", "quart", "qts", "qt", "gallons", "gallon", "gals", "gal", "pounds", "pound", "lbs", "lb", "large", "small", "medium")
meas <- lapply(meas, function(x) paste0(" ", x, " "))
meas_patt_string <- paste(unlist(meas), collapse = "|")
ar_df_long_neg_ING <- ar_df_long_neg |>
    mutate(Review_Txt = tolower(Review_Txt),
           Review_Txt = paste0(" ", Review_Txt, " "),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = "’",
                                        replacement = "'"),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = "(\\d+)\\.(\\d+)",
                                        replacement = "\\1point\\2"),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = "(\\d+)/(\\d+)",
                                        replacement = "\\1slash\\2"),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = "[^[:alnum:][:space:]']",
                                        replacement = " "),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = num_patt_string,
                                        replacement = " __NUM__ "),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = ing_patt_string,
                                        replacement = " __ING__ "),
           Review_Txt = str_replace_all(Review_Txt,
                                        pattern = meas_patt_string,
                                        replacement = " __MEAS__ "),
           Review_Txt = str_squish(Review_Txt))

```

We tokenize the refined text into unigrams.

```{r }
keep <- c("URL", "Review_Num", "Review_Txt")
ar_df_long_neg_unigrams <- ar_df_long_neg_ING |>
    select(all_of(keep)) |>
    unnest_tokens(output = UNIGRAM, input = Review_Txt,
                  token = "words", to_lower = FALSE, strip_punct = FALSE) |>
    count(URL, Review_Num, UNIGRAM, name = "COUNT") |>
    ungroup() |>
    filter(UNIGRAM != "'")

```

```{r }
sel <- c("ID", prob)
ar_df_long_neg_labs_only_num <- ar_df_long_neg_labs |>
    unite("ID", URL, Review_Num, sep = "_Review_Num_") |>
    select(all_of(sel)) |>
    arrange(ID)
ar_df_long_neg_labs_only <- ar_df_long_neg_labs_only_num |>
    mutate(across(all_of(prob), ~ factor(., levels = c(0,1),
                                         labels = c("No", "Yes"))))
ar_df_long_neg_unigrams_dtm <- ar_df_long_neg_unigrams |>
    unite("ID", URL, Review_Num, sep = "_Review_Num_") |>
    cast_dtm(ID, UNIGRAM, COUNT)
ar_df_long_neg_unigrams_sparse <- tidy(ar_df_long_neg_unigrams_dtm) |>
    right_join(ar_df_long_neg_labs_only |> select(ID),
               by = join_by(document == ID)) |>
    arrange(document) |>
    mutate(term = ifelse(is.na(term), "***__***", term),
           count = ifelse(is.na(count), 1, count)) |>
    cast_sparse(document, term, count)

```



```{r }
set.seed(1006)
sample <- sample(nrow(ar_df_long_neg_unigrams_sparse),
                 round(nrow(ar_df_long_neg_unigrams_sparse) * 0.5),
                 replace = FALSE)
train_x <- ar_df_long_neg_unigrams_sparse[sample, ]
train_y <- ar_df_long_neg_labs_only[sample, ]
train_y_num <- ar_df_long_neg_labs_only_num[sample, ]
validate_test_x <- ar_df_long_neg_unigrams_sparse[-sample, ]
validate_test_y <- ar_df_long_neg_labs_only[-sample, ]
validate_test_y_num <- ar_df_long_neg_labs_only_num[-sample, ]
sample <- sample(nrow(validate_test_x),
                 round(nrow(validate_test_x) * 0.5),
                 replace = FALSE)
validate_x <- validate_test_x[sample, ]
validate_y <- validate_test_y[sample, ]
validate_y_num <- validate_test_y_num[sample, ]
test_x <- validate_test_x[-sample, ]
test_y <- validate_test_y[-sample, ]
test_y_num <- validate_test_y_num[-sample, ]

```

## Model Development

```{r }
fns <- c("nb1.rds", "nb2.rds", "nb3.rds", "nb4.rds", "nb5.rds", "nb6.rds",
         "nb7.rds")
nbs <- list()
for (i in 1:length(fns)){
    if (!file.exists(fns[i])){
        col <- i + 1
        nb <- multinomial_naive_bayes(train_x, train_y[, col], laplace = 1)
        saveRDS(nb, fns[i])
    }else{
        nb <- readRDS(fns[i])
    }
    nbs[[i]] <- nb
}

```

```{r }
fns <- c("svm1.rds", "svm2.rds", "svm3.rds", "svm4.rds", "svm5.rds", "svm6.rds",
         "svm7.rds")
svms <- list()
ctrl <-  tune.control(sampling = "cross", cross = 10, nrepeat = 1)
tune_grid <- list(cost = c(0.1, 1, 10, 100, 1000))
for (i in 1:length(fns)){
    if (!file.exists(fns[i])){
        col <- i + 1
        svm_tune <- tune(svm, train.x = train_x, train.y = train_y[, col],
                         kernel = "linear", ranges = tune_grid,
                         tunecontrol = ctrl)
        svm_mod <- svm_tune$best.model
        saveRDS(svm_mod, fns[i])
    }else{
        svm_mod <- readRDS(fns[i])
    }
    svms[[i]] <- svm_mod
}

```

```{r }
fns <- c("xgb1.rds", "xgb2.rds", "xgb3.rds", "xgb4.rds", "xgb5.rds", "xgb6.rds",
         "xgb7.rds")
xgbs <- list()
for (i in 1:length(fns)){
    if (!file.exists(fns[i])){
        col <- i + 1
        xgb <- xgboost(train_x, train_y_num[, col], nrounds = 100,
                       objective = "binary:hinge", verbose = 0)
        saveRDS(xgb, fns[i])
    }else{
        xgb <- readRDS(fns[i])
    }
    xgbs[[i]] <- xgb
}

```

## Model Evaluation

```{r }
preds <- list()
cms_complete <- list()
cms <- list()
for (i in 1:length(nbs)){
    nb <- nbs[[i]]
    nb_pred <- predict(nb, validate_x, type = "class")
    preds[[i]] <- nb_pred
    col <- i + 1
    nbcm_complete <- confusionMatrix(nb_pred, validate_y[, col],
                                     positive = "Yes")
    cms_complete[[i]] <- nbcm_complete
    nbcm <- as.data.frame(nbcm_complete$table)
    nbcm$Reference <- factor(nbcm$Reference,
                             levels = rev(levels(nbcm$Reference)))
    nbcm <- nbcm |>
        mutate(
            Label = case_when(
                Prediction == "No" & Reference == "No" ~ "TN",
                Prediction == "Yes" & Reference == "Yes" ~ "TP",
                Prediction == "No" & Reference == "Yes" ~ "FN",
                Prediction == "Yes" & Reference == "No" ~ "FP"),
            Model = prob[i])
    cms[[i]] <- nbcm
}
cm <- bind_rows(cms)
p6 <- cm |>
    ggplot(aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(col = "black") +
    geom_text(aes(label = Freq), vjust = -1) +
    geom_text(aes(label = Label), vjust = 1) + 
    scale_fill_gradient(low = "white", high = pal[4]) +
    scale_x_discrete(position = "top") +
    facet_wrap(Model ~ ., ncol = 5, strip.position = "bottom") +
    labs(title = "Confusion Matrices for All Naive Bayes Models") +
    theme(axis.line.x = element_blank(),
          axis.line.y = element_blank(),
          axis.text.y = element_text(angle = 90, hjust = 0.5),
          axis.ticks = element_blank(),
          legend.position = "bottom",
          strip.placement = "outside")
p6

```

```{r warning = FALSE, message = FALSE}
by_class <- list()
overall <- list()
for (i in 1:length(cms_complete)){
    cm_complete <- cms_complete[[i]]
    by_class[[i]] <- as.data.frame(cm_complete$byClass)
    overall[[i]] <- as.data.frame(cm_complete$overall)
}
by_class <- t(round(bind_cols(by_class), 4))
rownames(by_class) <- prob
overall <- t(round(bind_cols(overall), 4))
rownames(overall) <- prob
metrics <- as.data.frame(cbind(by_class, overall))
keep <- c("Accuracy", "Kappa", "Precision", "Recall", "F1", "Specificity")
metrics <- metrics |>
    select(all_of(keep))
kable(metrics, format = "simple")

```

```{r }
preds <- list()
cms_complete <- list()
cms <- list()
for (i in 1:length(svms)){
    svm_mod <- svms[[i]]
    svm_pred <- predict(svm_mod, validate_x, type = "class")
    preds[[i]] <- svm_pred
    col <- i + 1
    svmcm_complete <- confusionMatrix(svm_pred, validate_y[, col],
                                      positive = "Yes")
    cms_complete[[i]] <- svmcm_complete
    svmcm <- as.data.frame(svmcm_complete$table)
    svmcm$Reference <- factor(svmcm$Reference,
                             levels = rev(levels(svmcm$Reference)))
    svmcm <- svmcm |>
        mutate(
            Label = case_when(
                Prediction == "No" & Reference == "No" ~ "TN",
                Prediction == "Yes" & Reference == "Yes" ~ "TP",
                Prediction == "No" & Reference == "Yes" ~ "FN",
                Prediction == "Yes" & Reference == "No" ~ "FP"),
            Model = prob[i])
    cms[[i]] <- svmcm
}
cm <- bind_rows(cms)
p7 <- cm |>
    ggplot(aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(col = "black") +
    geom_text(aes(label = Freq), vjust = -1) +
    geom_text(aes(label = Label), vjust = 1) +
    scale_fill_gradient(low = "white", high = pal[4]) +
    scale_x_discrete(position = "top") +
    facet_wrap(Model ~ ., ncol = 5, strip.position = "bottom") +
    labs(title = "Confusion Matrices for All Support Vector Machine Models") +
    theme(axis.line.x = element_blank(),
          axis.line.y = element_blank(),
          axis.text.y = element_text(angle = 90, hjust = 0.5),
          axis.ticks = element_blank(),
          legend.position = "bottom",
          strip.placement = "outside")
p7


```

```{r warning = FALSE, message = FALSE}
by_class <- list()
overall <- list()
for (i in 1:length(cms_complete)){
    cm_complete <- cms_complete[[i]]
    by_class[[i]] <- as.data.frame(cm_complete$byClass)
    overall[[i]] <- as.data.frame(cm_complete$overall)
}
by_class <- t(round(bind_cols(by_class), 4))
rownames(by_class) <- prob
overall <- t(round(bind_cols(overall), 4))
rownames(overall) <- prob
metrics <- as.data.frame(cbind(by_class, overall))
keep <- c("Accuracy", "Kappa", "Precision", "Recall", "F1", "Specificity")
metrics <- metrics |>
    select(all_of(keep))
kable(metrics, format = "simple")

```

```{r }
preds <- list()
cms_complete <- list()
cms <- list()
for (i in 1:length(xgbs)){
    xgb <- xgbs[[i]]
    xgb_pred <- predict(xgb, validate_x, type = "class")
    preds[[i]] <- xgb_pred
    col <- i + 1
    xgbcm_complete <- confusionMatrix(as.factor(xgb_pred),
                                      as.factor(validate_y_num[, col]),
                                      positive = "1")
    cms_complete[[i]] <- xgbcm_complete
    xgbcm <- as.data.frame(xgbcm_complete$table)
    xgbcm$Reference <- factor(xgbcm$Reference,
                             levels = rev(levels(xgbcm$Reference)))
    xgbcm <- xgbcm |>
        mutate(
            Label = case_when(
                Prediction == 0 & Reference == 0 ~ "TN",
                Prediction == 1 & Reference == 1 ~ "TP",
                Prediction == 0 & Reference == 1 ~ "FN",
                Prediction == 1 & Reference == 0 ~ "FP"),
            Model = prob[i])
    cms[[i]] <- xgbcm
}
cm <- bind_rows(cms)
p8 <- cm |>
    ggplot(aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(col = "black") +
    geom_text(aes(label = Freq), vjust = -1) +
    geom_text(aes(label = Label), vjust = 1) + 
    scale_fill_gradient(low = "white", high = pal[4]) +
    scale_x_discrete(position = "top") +
    facet_wrap(Model ~ ., ncol = 5, strip.position = "bottom") +
    labs(title = "Confusion Matrices for All eXtreme Gradiant Boosting Models") +
    theme(axis.line.x = element_blank(),
          axis.line.y = element_blank(),
          axis.text.y = element_text(angle = 90, hjust = 0.5),
          axis.ticks = element_blank(),
          legend.position = "bottom",
          strip.placement = "outside")
p8

```

```{r warning = FALSE, message = FALSE}
by_class <- list()
overall <- list()
for (i in 1:length(cms_complete)){
    cm_complete <- cms_complete[[i]]
    by_class[[i]] <- as.data.frame(cm_complete$byClass)
    overall[[i]] <- as.data.frame(cm_complete$overall)
}
by_class <- t(round(bind_cols(by_class), 4))
rownames(by_class) <- prob
overall <- t(round(bind_cols(overall), 4))
rownames(overall) <- prob
metrics <- as.data.frame(cbind(by_class, overall))
keep <- c("Accuracy", "Kappa", "Precision", "Recall", "F1", "Specificity")
metrics <- metrics |>
    select(all_of(keep))
kable(metrics, format = "simple")

```



## Conclusion